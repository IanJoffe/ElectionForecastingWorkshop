{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting the 2022 Election: A PCS Workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by Ian Joffe, Spring 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Video\n",
    "%store -r solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Empirical Facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Polls are pretty good - on average they're only off by only 3-5%\n",
    "2. Polls are only pretty good - on average they're off by 3-5%\n",
    "3. While individual polls have their flaws, the polling average doens't favor either side, and polling error in previous elections doesn't correlate with polling error in the next election. In a statistical sense, we can call the polling average \"unbiased\" with respect to the election result. \n",
    "\n",
    "Also emprically, polls have been by far the best predictor of election outcomes. In the forecast that we're building, we will use the polling average as our *only* input.\n",
    "\n",
    "We begin by looking at the generic ballot polling average, which is the percent of Americans who say they want to vote Democrats or Republicans into congress. There is evidence that districts tend to \"track\" the generic ballot, i.e. if the generic ballot moves a point to one side, each district in the country will tend to also move about a point in that direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the Beta Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we assume the generic ballot polling average is unbiased, it's easy to predict the winner of an election (or at least, the winner of the majority of votes). Our goal is to determine exactly how certain we are of that the predicted winner will previal. \n",
    "\n",
    "A model approximates a function. Our model input will be the generic ballot polling average for democrats (a proportion of voters from 0 to 1), by convention. Our model output will be a distribution on all possible election day results for demoracts (proportions of true votes they receive). \n",
    "\n",
    "It's too hard to look at all possible functions that transform the the polling average into a distribution of outcomes. The point of a model is to is to specify a set of functions that we will consider. For our model, we'll assume that the distribution of outcomes follows a beta distribution, which is useful because the beta's domain is $[0,1]$, so all possible values are a possible proportion of votes. \n",
    "\n",
    "The beta distribution is an example of a [continuous](http://prob140.org/textbook/content/Chapter_15/00_Continuous_Distributions.html) probability [distribution](http://prob140.org/textbook/content/Chapter_03/02_Distributions.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 1\n",
    "\n",
    "x = np.linspace(0,1,200)\n",
    "y = stats.beta.pdf(x, alpha, beta)\n",
    "plt.plot(x, y);\n",
    "plt.title(\"Beta(\" + str(alpha) + \", \" + str(beta) + \")\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the affect of increasing $\\alpha$? What is the affect of increasing $\\beta$? What happens if we set $\\alpha = \\beta$ and increase both? For $\\alpha, \\beta > 1$, approximately where is the distribution centered?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following cells must be run in order, once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use fivethirtyeight's generic ballot polling average for our forecast\n",
    "polls = pd.read_csv(\"generic_topline_historical.csv\")[[\"modeldate\", \"dem_estimate\", \"rep_estimate\"]]\n",
    "polls[\"dem_lead\"] = polls[\"dem_estimate\"] - polls[\"rep_estimate\"]\n",
    "polls = polls.drop([\"dem_estimate\", \"rep_estimate\"], axis=1)\n",
    "polls = polls[polls[\"modeldate\"].str.slice(0,1) == \"4\"]\n",
    "print(\"All April Polls:\")\n",
    "polls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polls[\"year\"] = polls[\"modeldate\"].str.slice(start=-4).astype(int)\n",
    "polls = polls[np.mod(polls[\"year\"], 2) == 0]\n",
    "polls = polls[polls[\"year\"] % 2 == 0]\n",
    "polls = polls.groupby(\"year\").mean()\n",
    "# I have to manually add data from 2018 and 2020 because it's from a different data set\n",
    "additional_data = pd.DataFrame({\"Year\": [2018, 2020], \"dem_lead\": [7.1093803, 7.749738]})\n",
    "additional_data = additional_data.set_index(\"Year\")\n",
    "polls = polls.append(additional_data)\n",
    "# Append the final voting margins for each year to the data set\n",
    "final_margins = [0, -1, 0, -5, -3, 8, 11, -7, 1, -6, 1, 9, 3]\n",
    "polls[\"true_result\"] = final_margins\n",
    "polls[\"dem_lead\"] = polls[\"dem_lead\"] / 100\n",
    "polls[\"true_result\"] = polls[\"true_result\"] / 100\n",
    "print(\"April Polling Averages from Midterm Years\")\n",
    "polls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polls_abs = polls.copy(deep=True)\n",
    "polls_abs[\"dem_lead\"] = .5 + polls[\"dem_lead\"] / 2\n",
    "polls_abs[\"true_result\"] = .5 + polls[\"true_result\"] / 2\n",
    "polls_abs = polls_abs.rename(columns={\"dem_lead\": \"dem_pct\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\alpha$ and $\\beta$ parameters of the beta distribution aren't very interpretable in the context of election forecasting. It would be really nice if we could specify the polling average as one parameter, which will center our distribution of outcomes, and then have another parameter that specifies how certain/uncertain we are that the polling average will reflect the election results. \n",
    "\n",
    "Luckily, in the beta's case this isn't too hard. With a few algebric moves, we can make the beta's distribution a function of a given \"mode\" and \"concentration.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_reparam(x, mode, concentration):\n",
    "    return stats.beta.pdf(x, mode * (concentration - 2) + 1, (1 - mode) * (concentration - 2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = .5\n",
    "concentration = 300\n",
    "\n",
    "x = np.linspace(0,1,200)\n",
    "y = beta_reparam(x, mode, concentration)\n",
    "plt.plot(x, y);\n",
    "plt.title(\"Beta(mode=\" + str(mode) + \", concentration=\" + str(concentration) + \")\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, experiment with the affects of changing the mode and concentration parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building our model! Our model specifies the probability of an election result given a polling average. To build a good model, we want true historical election results to be highly probable given our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```calculate_result_probabilities``` returns a list of the probabilities of the true outcome for each election since 1996, with respect to their polling leads and a given confidence in polling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_result_probabilities(confidence):\n",
    "    true_results = polls_abs[\"true_result\"].values\n",
    "    leads = polls_abs[\"dem_pct\"].values\n",
    "    n = len(true_results)\n",
    "    return np.array([beta_reparam(...[i], ...[i], ...) for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run to view solution\n",
    "print(solutions[\"calculate_result_probabilities\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to determine model good-ness, we design on function that looks at our model, and returns one output. A [loss function](https://www.textbook.ds100.org/ch/17/risk_intro.html) return a lower number for a better model. \n",
    "\n",
    "Keep in mind that all the models we consider are beta distributions, and they have a fixed mode - the generic ballot polling average. So, given the set of possible functions, just giving the confidence parameter specifies one possible function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we choose some value for confidence. Design an appropriate loss function that calculates the loss for the chosen confidence (and therefore the chosen function) over elections since 1996. Your loss function should call ```calculate_result_probabilities(confidence)```\n",
    "\n",
    "Hint: A loss function could return a \"more negative\" number to represent a better model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(confidence):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function for many different confidence values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = np.linspace(0, 2000, 500)\n",
    "losses = []\n",
    "for c in confs:\n",
    "    losses.append(loss(c))\n",
    "plt.plot(confs, losses);\n",
    "plt.xlabel(\"Confidence Parameter Value\");\n",
    "plt.ylabel(\"Loss\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to view solution\n",
    "print(solutions[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The machine learning workflow has three general steps: \n",
    "1. Specify your model\n",
    "2. Define a loss that determines the good-ness of your model\n",
    "3. Optimize your model to minimize your loss\n",
    "\n",
    "We've just finished steps 1 and 2. To find the parameter value (in this case, the confidence value) that optimizes our loss, we'll use a method called [gradient descent](https://www.textbook.ds100.org/ch/20/gradient_descent_define.html). Gradient descent starts with a random value for the parameter, and then updates it over and over again, at each step nudging it in the direction of the optimal parameter. At each step, we update the parameter $\\theta$ according to the update equation using the loss function's derivative at the current value of $\\theta$:\n",
    "$\\begin{align}\n",
    "\\theta \\leftarrow \\theta - \\alpha\\frac{dL}{d\\theta}(\\theta)\n",
    "\\end{align}$\n",
    "\n",
    "The \"step size\" $\\alpha$ determines how \"large\" our update in each step will be. \n",
    "\n",
    "We'll use the method of finite difference, defined below as `loss_deriv_df` to calculate the loss function's derivative at a certain confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the method of finite differences to calculate the derivative of our loss function at a given confidence\n",
    "\n",
    "def finite_difference(func, x, epsilon=.0001):\n",
    "    return (func(x+epsilon) - func(x-epsilon)) / (2 * epsilon)\n",
    "\n",
    "def loss_deriv_fd(confidence, epsilon=.001):\n",
    "    return finite_difference(loss, confidence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the Gradient Descent algorithm. Line 1 randomly initializes our initial confidence. Line 2 sets the number of \"epochs,\" which is the number of times we go over all our data, calculate a new loss, and reassign ```optimal_confidence```. Line 3 sets the learning rate - you can try messing with this, but it's preset to a good value. Write one line to replace the elipses which updates ```optimal_confidence``` according to the gradient descent formula. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_confidence = np.random.uniform(1, 100)\n",
    "epochs = 500\n",
    "learning_rate = 10\n",
    "\n",
    "optimal_confidence = initial_confidence\n",
    "for epoch in range(epochs):\n",
    "    ...\n",
    "    \n",
    "print(optimal_confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solutions[\"gradient descent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've officially finished building our model for the generic ballot! Having found the optimal confidence parameter, we now have a fully parameterized beta distribution that will show the probability of democrats receiving any proportion of votes in the country. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, we use FiveThiryEight's metrics for paritsan lean and elasticity.\n",
    "# The partisan lean metric is on the correct scale for our purpose, but the elasticity metric is normalized around 1. \n",
    "# This function takes in a FiveThirtyEight elasticity value, and applies some transformation to it to make it more useful in \n",
    "# our context. \n",
    "def elasticity_to_confidence(elasticity):\n",
    "    return 500 - 1000 * (elasticity - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The current percent of the vote going to democrats, adapted from FiveThirtyEight's 2022 generic ballot tracker, \n",
    "# as of this morning (4/17). Recall we assume that all votes go to dem or rep, so dem_pct = 0.5 + dem_lead / 2\n",
    "current_dem_pct = .4885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the FiveThirtyEight's data about each district. \n",
    "# Here, we apply another simplifcation. Since not all district maps have been approved, we assume the 2022 house districts\n",
    "# and their partisan leans match those of 2020. \n",
    "districts = pd.read_csv(\"partisan_leans.csv\", header=None).loc[:,[0,1,2]].rename(columns={0:\"district\", 1:\"lean\", 2:\"elasticity\"})\n",
    "print(\"The FiveThirtyEight partisan lean and elasticity of each district:\")\n",
    "districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the scale of the lean metric to match our standard\n",
    "districts[\"lean\"] = districts[\"lean\"].str.slice(2).astype(float) * np.where(districts[\"lean\"].str.slice(stop=1) == \"D\", 1, -1) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the generic ballot is useful, but winning the generic ballot isn't actually worth anything in U.S. politics. We now aim to deterimine the probability that democrats win or lose the House, and if so by how many seats. \n",
    "\n",
    "As our task gets more complicated, it becomes harder to model with a strictly defined known probability distribution. Instead, we'll run 500 simulations of election night, and use the simulation results to estimate the probability of Democrats keeping the house. For this part, we'll utilize Emprical Fact #4 about: \n",
    "\n",
    "4. Districts tend to track the generic ballot average. \n",
    "\n",
    "I mentioned this earlier, but will explain in more specifically in the context of our House forecast here. Each district has a partisan lean, it's expected distance from the generic ballot average. Partisan lean also involves uncertainty - sometimes a district is more left- or right-leaning than it has been historically. We call a district's uncertainty in partisan lean the district's *elasticity*: a more elastic district is more likely to sway away from it's historic partisan lean. Partisan lean and elasticity are both calculated by fivethirtyeight. \n",
    "\n",
    "Here's how each of our House simulations will work:\n",
    "1. Take a random draw from the beta we just optimized to get the proportion of voters that support Democrats on election day. \n",
    "2. For each district, add the partisan lean to the randomized election day result. This is the district's expected, or mode, number of voters for democrats. The certainity (or confidence) in this mode is the district's elasticy... see where I'm going with this? How will we randomly draw a result for the district? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Q4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the values of ```mode``` and ```concentration``` for the simulation of a singular district under the beta distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_district(natl_gen_ballot, state_lean, elasticity):\n",
    "    mode = ...\n",
    "    concentration = ...\n",
    "    if mode > 1:\n",
    "        return 1\n",
    "    elif mode < 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return stats.beta.rvs(mode * (concentration - 2) + 1, (1 - mode) * (concentration - 2) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solutions[\"simulate_district\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simulation for the entire country. Note how here, the mode is set to the current generic ballot lead for democrats, and the concentration is the set to the optimal concentration we set earlier. This function calls ```simulate_district``` for each district, keeps a list of the probability of democrats winning each district, and returns the number of districts in which democrats are at least 50% likely to win. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_country():\n",
    "    mode = current_dem_lead\n",
    "    concentration = optimal_confidence\n",
    "    natl_avg = stats.beta.rvs(mode * (concentration - 2) + 1, (1 - mode) * (concentration - 2) + 1)\n",
    "    dist_outcomes = []\n",
    "    for district in districts.iterrows():\n",
    "        dist_outcomes.append(simulate_district(natl_avg, district[1][\"lean\"], elasticity_to_confidence(district[1][\"elasticity\"])))\n",
    "    # Returns the number of seats that democrats \"win\"\n",
    "    return sum(np.array(dist_outcomes) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run simulate_country ```n_simulations``` times, and make a list of the seats that democrats win each of them. In the next cell, we make a histogram of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 500\n",
    "dem_seats = []\n",
    "for i in range(n_simulations):\n",
    "    dem_seats.append(simulate_country())\n",
    "dem_seats = np.array(dem_seats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(dem_seats, bins=20);\n",
    "plt.axvline(218, color=\"red\", linewidth=1);\n",
    "plt.title(\"Probability of Dem Majority: \" + str(sum(dem_seats >= 218) / len(dem_seats)) + \"\\nMean Dem Seats: \" + str(int(np.mean(dem_seats))));\n",
    "plt.xlabel(\"Dem Seats\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You've built a model to forecast the probability of Democrats keeping the House in 2022, based on April polls. This was a long process and requires computationally time-consuming simulations. Could be find a much simpler method to get about the same result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Much More Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Q5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tipping point district where, if all more conservative districts go red, and all more liberal districts go blue, the House will be tied.\n",
    "\n",
    "In an attempt to simplify our model, we say that the house goes whatever way the tipping point district goes. The following code block should execute this strategy rather efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the partisan lean of the tipping point district\n",
    "# Recall the we can get a list of all partisan leans with districts[\"lean\"]\n",
    "# Perhaps google a function you can apply to this list (or, more accurately, pandas series)?\n",
    "tipping_point_dist_lean = ...\n",
    "\n",
    "print(\"The tipping point district has a lean of \" + str(tipping_point_dist_lean))\n",
    "\n",
    "# Now, we plan to plug this district's mode and concentration into the beta distribution.\n",
    "# What would be use for the mode?\n",
    "tipping_point_dist_mode = ...\n",
    "\n",
    "tipping_point_dist_concentration = optimal_concentration\n",
    "\n",
    "# Convert back to the alpha/beta paramaterization and plug in to the beta distribution\n",
    "alpha = tipping_point_dist_mode * (tipping_point_dist_concentration - 2) + 1\n",
    "beta = (1 - tipping_point_dist_mode) * (tipping_point_dist_concentration - 2) + 1\n",
    "prob_dem_win = 1 - stats.beta.cdf(.5, alpha, beta)\n",
    "print(\"According to the simple model, Democrats have a \" + str(prob_dem_win) + \" chance of winning the house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solutions[\"tipping_point_model\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should give a pretty similar result to our original model. Often, simpler models are more performative than complex ones on real-world data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Slightly More Complex Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The \"Midterm Penalty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I lied just a little bit at the beginning. It's true that polls are not empirically biased towards either party in particular, and it's also true that polls right before election day show no signs historically of bias. However in Midterm years, April polls often show better results for the President's party compared to election day results. It's possible that this is just because people change their opinions against the Presdient's party late, or because people who dedice late tend to decide against the President's party, but from the statistical modeling perspective we don't care about the reason (... or do we? Maybe we would care if we had a model that accounted for more factors). Let's add a final empirical fact: \n",
    "\n",
    "5. April polls are not necesserily unbiased with respect to the President's party\n",
    "\n",
    "Luckily, this isn't a big problem! We just have to account for it in our model, again using historic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we add the ```dem_pres``` column to the polls table, which is is 0 in general elections years, 1 if the sitting president is a democrat, and -1 if the sitting president is a republican. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polls_abs[\"dem_pres\"] = [0, 1, 0, -1, 0, -1, 0, 1, 0, 1, 0, -1, 0]\n",
    "polls_abs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model's confidence parameter is still the same this conceptually, but we'll have to recalculate it's optimal value. The difference is that now, the mode parameter is the generic ballot average *minus* a midterm penality. We'll apply the machine learning method to learn the optimal value to use for the midterm penalty. Our model is still the beta distribution, just with the new mode, and our loss function can stay the same. Let's re-optimize. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Q6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our setup for gradient descent is very similar to before, but now we have two parameters to optimize (the confidence and the penalty) instead of one (just the confidence) like before. Loss is a function of both parameters, and it has two derivatives: one with respect to each parameter. Like before, we use the finite difference method to calculate each derivative. Complete the two finite difference functions to find each of the two derivatives. You can use a non-general version of the old univariate finite difference function as a model: \n",
    "\n",
    "```\n",
    "def finite_difference(confidence, epsilon=.0001):\n",
    "    return (loss(confidence+epsilon) - loss(confidence-epsilon)) / (2 * epsilon)\n",
    "```\n",
    "\n",
    "In each of the new functions we should be applying the epsilon transformation to one of the two parameters, and keeping the other one constant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_result_probabilities(confidence, penalty):\n",
    "    true_results = polls_abs[\"true_result\"].values\n",
    "    leads = polls_abs[\"dem_lead\"].values\n",
    "    dem_pres = polls_abs[\"dem_pres\"].values\n",
    "    n = len(true_results)\n",
    "    # Take a look at this line\n",
    "    # Our new mode is now the generic ballot (like you set up in Code Q1) MINUS a penalty, which is applied to\n",
    "    # niether party in a general election year, and the sitting presidential party in midterm years. \n",
    "    return np.array([beta_reparam(true_results[i], leads[i] - penalty*dem_pres[i], confidence) for i in range(n)])\n",
    "\n",
    "def loss(confidence, penalty):\n",
    "    return -sum(calculate_result_probabilities(confidence, penalty) ** (1/3))\n",
    "\n",
    "def loss_deriv_wrt_confidence(confidence, penalty, epsilon=.001):\n",
    "    def finite_difference(confidence, penalty):\n",
    "        return (loss(..., ...) - loss(..., ...)) / (2 * epsilon)\n",
    "    return finite_difference(confidence, penalty)\n",
    "\n",
    "def loss_deriv_wrt_penalty(confidence, penalty, epsilon=.001):\n",
    "    def finite_difference(confidence, penalty):\n",
    "        return (loss(..., ...) - loss(..., ...)) / (2 * epsilon)\n",
    "    return finite_difference(confidence, penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(solutions[\"multivariate fd\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform gradient descent. This is just a doubled version of the algorithm that you wrote earlier. We initialize *both* parameters, and then apply the update step to *both* parameters, calling each's own derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_confidence = np.random.uniform(1, 100)\n",
    "initial_penalty = np.random.uniform(0, .1)\n",
    "epochs = 500\n",
    "learning_rate_confidence = 10\n",
    "learning_rate_penalty = .0001\n",
    "\n",
    "optimal_confidence = initial_confidence\n",
    "optimal_penalty = initial_penalty\n",
    "for epoch in range(epochs):\n",
    "    optimal_confidence -= learning_rate_confidence * loss_deriv_wrt_confidence(optimal_confidence, optimal_penalty)\n",
    "    optimal_penalty -= learning_rate_penalty * loss_deriv_wrt_penalty(optimal_confidence, optimal_penalty)\n",
    "    \n",
    "print(optimal_confidence)\n",
    "print(optimal_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_district(natl_gen_ballot, state_lean, elasticity):\n",
    "    mode = natl_gen_ballot + state_lean\n",
    "    concentration = elasticity\n",
    "    if mode > 1:\n",
    "        return 1\n",
    "    elif mode < 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return stats.beta.rvs(mode * (concentration - 2) + 1, (1 - mode) * (concentration - 2) + 1)\n",
    "    \n",
    "def simulate_country():\n",
    "    # The only change from earlier is the following line.\n",
    "    # The midterm penalty is applied to democrats because Joe Biden is president. \n",
    "    mode = current_dem_lead - optimal_penalty\n",
    "    concentration = optimal_confidence\n",
    "    natl_avg = stats.beta.rvs(mode * (concentration - 2) + 1, (1 - mode) * (concentration - 2) + 1)\n",
    "    dist_outcomes = []\n",
    "    for district in districts.iterrows():\n",
    "        dist_outcomes.append(simulate_district(natl_avg, district[1][\"lean\"], elasticity_to_confidence(district[1][\"elasticity\"])))\n",
    "    return sum(np.array(dist_outcomes) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, simulate and graph the election results ```n_simulations``` times. Though you may encounter some randomness, Democrats should generally do worse in this model than earlier due to the penalty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 500\n",
    "dem_seats = []\n",
    "for i in range(n_simulations):\n",
    "    dem_seats.append(simulate_country())\n",
    "dem_seats = np.array(dem_seats)\n",
    "\n",
    "plt.hist(dem_seats, bins=20);\n",
    "plt.axvline(218, color=\"red\", linewidth=1);\n",
    "plt.title(\"Probability of Dem Majority: \" + str(sum(dem_seats >= 218) / len(dem_seats)) + \"\\nMean Dem Seats: \" + str(int(np.mean(dem_seats))));\n",
    "plt.xlabel(\"Dem Seats\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariant Districts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conceptual Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some distircts are likely to vote similarly to others, often based on geographic or demographic factors. Improve your model by accounting for this in the ```covariant_districts``` dictionary. Each key district maps to one or multiple value districts, which its outcomes correlate to. Using your knowledge of geography and demography (plus http://proximityone.com/cd.htm, a resource which has demographic info on congrssional districts at a table in the middle, and https://www.govtrack.us/congress/members/map, a map of US congressional districts), improve the model by accounting for covariant districts. I've already added districts from downtown NYC, Los Angeles, and Chicago - although I won't guarantee that they are actually correlated, you should check that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariant_districts = {\"CA-34\": [\"NY-12\", \"IL-7\"], \"NY-12\": [\"CA-34\", \"IL-7\"], \"IL-7\":[\"CA-34\", \"NY-12\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is very similar to the functions we wrote earlier - the only difference is it now accounts for the covariant districts. First, we randomize the order of the districts. Then, for each district, if there are no covariants yet we do the same as before, but if there are covariants we apply them. \n",
    "\n",
    "The ```elasticity_factor```, which is between 0 and 1, is the factor by which we multiply the elasticity by for each covariant state. It makes sense that elasticity decreases for every state that is covariant. \n",
    "\n",
    "The ```average_factor``` is applied so that if a district correlates to one other district, the mode of its beta distribution is the national generic ballot times one minus the factor, plus the covariant state times the factor. \n",
    "\n",
    "You're welcome to play with these two parameters. In a much more complex model, each covariant will have its own value for these two factors, and those values could be optimized using a method like gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariant_state_elasticity_factor = .75\n",
    "covariant_state_average_factor = .3\n",
    "\n",
    "def simulate_district(natl_gen_ballot, district, prev_outcomes):\n",
    "    \n",
    "    def get_elasticity():\n",
    "        elasticity = elasticity_to_confidence(district[\"elasticity\"])\n",
    "        covariants = covariant_states.get(district[\"district\"][:2])\n",
    "        if covariants is not None:\n",
    "            for covariant in covariants:\n",
    "                if covariant in list(prev_outcomes.keys()):\n",
    "                    elasticity *= covariant_state_elasticity_factor\n",
    "        return elasticity\n",
    "        \n",
    "    def get_avg():\n",
    "        avg = natl_gen_ballot + district[\"lean\"]\n",
    "        covariants = covariant_states.get(district[\"district\"][:2])\n",
    "        if covariants is not None:\n",
    "            for covariant in covariants:\n",
    "                if covariant in list(prev_outcomes.keys()):\n",
    "                    avg = prev_outcomes[covariant] * convariant_state_average_factor + avg * (1-covariant_state_average_factor)\n",
    "        return avg\n",
    "            \n",
    "    mode = get_avg()\n",
    "    concentration = get_elasticity()\n",
    "    if mode > 1:\n",
    "        return 1\n",
    "    elif mode < 0:\n",
    "        return 0\n",
    "    else: \n",
    "        return stats.beta.rvs(mode * (concentration - 2) + 1, (1 - mode) * (concentration - 2) + 1)\n",
    "    \n",
    "def simulate_country():\n",
    "    mode = current_dem_lead\n",
    "    concentration = optimal_confidence\n",
    "    natl_avg = stats.beta.rvs(mode * (concentration - 2) + 1, (1 - mode) * (concentration - 2) + 1)\n",
    "    dist_outcomes = {}\n",
    "    for district in districts.iterrows():\n",
    "        dist_outcomes[district[1][\"district\"]] = simulate_district(natl_avg, district[1], dist_outcomes)\n",
    "    return sum(np.array(list(dist_outcomes.values())) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, run the simulations. Since we're only applying a few covariances you probably won't notice a big difference in the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 500\n",
    "dem_seats = []\n",
    "for i in range(n_simulations):\n",
    "    dem_seats.append(simulate_country())\n",
    "dem_seats = np.array(dem_seats)\n",
    "\n",
    "plt.hist(dem_seats, bins=20);\n",
    "plt.axvline(218, color=\"red\", linewidth=1);\n",
    "plt.title(\"Probability of Dem Majority: \" + str(sum(dem_seats >= 218) / len(dem_seats)) + \"\\nMean Dem Seats: \" + str(int(np.mean(dem_seats))));\n",
    "plt.xlabel(\"Dem Seats\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now written the key components of a working model for the 2022 house election! We made a lot of assumptions and simplifications that might affect our accuracy, but all in all, our model should actually be pretty decent. Once the industry standard models (e.g. FiveThirtyEight, The Economist) come out, it'll be interesting to see how close their probabilities match up with ours.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update, October 2022: What an interesting Midterm cycle it has been! When the industrial models first came out in April they looked pretty similar to ours, but that all changed when the Supreme Court overturned Roe v. Wade, allowing states to ban abortion. Democrats rose in the generic ballot, and their chances have risen in models, which now give them a 30-40% chance to win in the house. This doesn't mean our model is miscalibrated - we include uncertainty because crazy things happen! You could say that we were right, and that in April's perspective Democrats did only have a 5-10% chance of keeping the house. But, this is impossible to test since we can't create hundreds of possible American futures and see how if Democrats win the house in the predicted number of them. And, who's to say what's to come in the next month of polling, events may spark a new swing back to the April polling average, or further from it.\n",
    "\n",
    "That said, the Supreme Court's docket is public knowledge? Is there a way we could factor this into a more complicated model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(\"polls.mov\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
